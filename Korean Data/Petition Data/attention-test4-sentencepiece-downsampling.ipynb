{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from konlpy.tag import Kkma, Okt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import kss\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sn\n",
    "import matplotlib.font_manager as fm\n",
    "fm.get_fontconfig_fonts()\n",
    "font_location = '/home/bigdata/jupyter_notebook/wontae/NanumGothic.ttf' # For Windows\n",
    "font_name = fm.FontProperties(fname=font_location)\n",
    "plt.rc('font', family = font_name.get_name())\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from keras import regularizers\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input, RepeatVector, Permute, Multiply, Lambda,BatchNormalization\n",
    "from keras.layers import Embedding, GRU, Bidirectional, TimeDistributed, LSTM, Dropout\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from nltk import tokenize\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, regularizer = None, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.regularizer = regularizer\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.context = self.add_weight(name='context', shape=(input_shape[-1], 1),initializer=initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None),regularizer=self.regularizer,trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        attention_in = K.exp(K.squeeze(K.dot(x, self.context), axis=-1))\n",
    "        attention = attention_in/K.expand_dims(K.sum(attention_in, axis=-1), -1)\n",
    "\n",
    "        if mask is not None:\n",
    "        # use only the inputs specified by the mask\n",
    "        # import pdb; pdb.set_trace()\n",
    "            attention = attention*K.cast(mask, 'float32')\n",
    "\n",
    "        weighted_sum = K.batch_dot(K.permute_dimensions(x, [0, 2, 1]), attention)\n",
    "        return weighted_sum\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  6 16:19:28 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 396.37                 Driver Version: 396.37                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |\r\n",
      "| N/A   30C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:D8:00.0 Off |                    0 |\r\n",
      "| N/A   27C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "petition_data = pd.read_csv('petition.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_content_combine(title,content):\n",
    "    return title + ' ' + str(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "petition_data['petition'] = petition_data.apply(lambda x : title_content_combine(x.title, x.content), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = petition_data[['petition', 'category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "정치개혁           62750\n",
       "기타             48682\n",
       "인권/성평등         34993\n",
       "안전/환경          30638\n",
       "교통/건축/국토       28184\n",
       "외교/통일/국방       26626\n",
       "육아/교육          25537\n",
       "보건복지           24848\n",
       "일자리            23225\n",
       "행정             20268\n",
       "문화/예술/체육/언론    18207\n",
       "미래             18000\n",
       "경제민주화          16793\n",
       "성장동력            7253\n",
       "반려동물            4057\n",
       "저출산/고령화대책       3584\n",
       "농산어촌            1902\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['category']).size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nb_instances = 100000\n",
    "x  = pd.DataFrame(data.petition)\n",
    "y = pd.DataFrame(data.category)\n",
    "y.columns = ['category']\n",
    "x_train = x[:max_nb_instances]\n",
    "y_train = y[:max_nb_instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace('\\n', '')\n",
    "    t = re.sub('[^0-9a-zA-Zㄱ-힗,.!?\"\"''\\s]', '', text)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['mod_petition'] = x.apply(lambda j : preprocess(j.petition), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x.mod_petition[:max_nb_instances]\n",
    "y_train = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "texts = []\n",
    "max_length_text = 0\n",
    "for text in x.mod_petition[:max_nb_instances]:\n",
    "    sentences =kss.split_sentences(text)\n",
    "    max_length_text = max(len(sentences), max_length_text)\n",
    "    for sen in sentences:\n",
    "        texts.append(sen)\n",
    "    reviews.append(sentences)\n",
    "print(max_length_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spm_input.txt', 'w', encoding='utf-8') as f:\n",
    "    for sent in texts:\n",
    "        f.write('{}\\n'.format(sent))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spm_input2.txt', 'w', encoding='utf-8') as f:\n",
    "    for sent in texts:\n",
    "        f.write('{}\\n'.format(sent))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.load('Hierarchical_Attention_Networks_Test/300k_vocab.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCES = 10\n",
    "MAX_SENTENCE_LENGTH = 25\n",
    "max_length_words =0 \n",
    "X_train = np.zeros((max_nb_instances, MAX_SENTENCES, MAX_SENTENCE_LENGTH), dtype='int32')\n",
    "result = []\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTENCES:\n",
    "            result.append(sp.EncodeAsPieces(sent))\n",
    "            wordTokens = sp.EncodeAsIds(sent)\n",
    "            max_length_words = max(len(wordTokens), max_length_words)\n",
    "            for k in range(len(wordTokens)):\n",
    "                if k< MAX_SENTENCE_LENGTH:\n",
    "                    X_train[i][j][k] = wordTokens[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_to_number(y_data):\n",
    "    category_list = list(set(y_data.category))\n",
    "    new_y = []\n",
    "    for i, category in enumerate(y_data.category):\n",
    "        new_y.append(category_list.index(category))\n",
    "    ret = pd.DataFrame(new_y)\n",
    "    ret.columns = ['category']\n",
    "    return ret,category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.columns = ['category']\n",
    "Y_train,category_list = category_to_number(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "16    17810\n",
       "9     12632\n",
       "2     10097\n",
       "5      8459\n",
       "0      8450\n",
       "4      5982\n",
       "15     5524\n",
       "10     5018\n",
       "13     4914\n",
       "11     4288\n",
       "12     4171\n",
       "7      4048\n",
       "8      3945\n",
       "6      2454\n",
       "1       931\n",
       "14      898\n",
       "3       379\n",
       "dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.groupby(['category']).size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>cnt</th>\n",
       "      <th>s_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8450</td>\n",
       "      <td>13.044736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>931</td>\n",
       "      <td>9.862637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10097</td>\n",
       "      <td>13.301639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>379</td>\n",
       "      <td>8.566054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5982</td>\n",
       "      <td>12.546412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>8459</td>\n",
       "      <td>13.046271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2454</td>\n",
       "      <td>11.260920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4048</td>\n",
       "      <td>11.982994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3945</td>\n",
       "      <td>11.945810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>12632</td>\n",
       "      <td>13.624795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>5018</td>\n",
       "      <td>12.292897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>4288</td>\n",
       "      <td>12.066089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>4171</td>\n",
       "      <td>12.026178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>4914</td>\n",
       "      <td>12.262682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>898</td>\n",
       "      <td>9.810572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>5524</td>\n",
       "      <td>12.431498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>17810</td>\n",
       "      <td>14.120400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y    cnt      s_cnt\n",
       "0    0   8450  13.044736\n",
       "1    1    931   9.862637\n",
       "2    2  10097  13.301639\n",
       "3    3    379   8.566054\n",
       "4    4   5982  12.546412\n",
       "5    5   8459  13.046271\n",
       "6    6   2454  11.260920\n",
       "7    7   4048  11.982994\n",
       "8    8   3945  11.945810\n",
       "9    9  12632  13.624795\n",
       "10  10   5018  12.292897\n",
       "11  11   4288  12.066089\n",
       "12  12   4171  12.026178\n",
       "13  13   4914  12.262682\n",
       "14  14    898   9.810572\n",
       "15  15   5524  12.431498\n",
       "16  16  17810  14.120400"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample = pd.DataFrame(Y_train)\n",
    "train_sample.columns = ['y']\n",
    "train_sample_y = train_sample.groupby('y').size().reset_index()\n",
    "train_sample_y.columns = ['y', 'cnt']\n",
    "train_sample_y\n",
    "train_sample_y['s_cnt'] = np.log2(train_sample_y.cnt)\n",
    "train_sample_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "for idx, row in train_sample_y.iterrows():\n",
    "    cand_x = X_train[(np.random.choice(Y_train[Y_train.y ==row.y].index, 100*int(row.s_cnt)))]\n",
    "    cand_y = [row.y]*(int(row.s_cnt)*100)\n",
    "    \n",
    "    train_x = train_x + list(cand_x)\n",
    "    train_y = train_y + cand_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = to_categorical(np.asarray(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.1\n",
    "nb_validation_samples = int(validation_split * len(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_x[:-nb_validation_samples]\n",
    "y_train = train_y[:-nb_validation_samples]\n",
    "x_val = train_x[-nb_validation_samples:]\n",
    "y_val = train_y[-nb_validation_samples:]\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    train_x, train_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Hierarchical_Attention_Networks_Test/300k_vocab.vocab', encoding='utf-8') as f:\n",
    "    Vo = [doc.strip().split(\"\\t\") for doc in f]\n",
    "    \n",
    "word2idx = {w[0]: i for i, w in enumerate(Vo)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = list(word2idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁대통령', 0.7808446884155273),\n",
       " ('▁여당', 0.7431647777557373),\n",
       " ('탄핵', 0.7428842782974243),\n",
       " ('▁문죄인', 0.7347427010536194),\n",
       " ('촛불', 0.7334913015365601),\n",
       " ('▁각료', 0.7313647866249084),\n",
       " ('민주당', 0.7306030988693237),\n",
       " ('▁임명한', 0.7303953170776367),\n",
       " ('▁대통령도', 0.7269872426986694),\n",
       " ('한국당', 0.7248755693435669)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('대통령')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "w2v_model = Word2Vec(result, size=embedding_dim, window=5, workers=4, sg=1, iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word, word2vec_model):\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(list(word_index))+1, embedding_dim))\n",
    "for word, i in enumerate(list(word2idx.keys())):\n",
    "    temp = get_vector(word, w2v_model)\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = regularizers.l2(1e-8)\n",
    "\n",
    "sentence_in = Input(shape=(MAX_SENTENCE_LENGTH,), dtype='int32')\n",
    "embedded_word_seq = Embedding(len(word_index) +1,embedding_dim, weights=[embedding_matrix], input_length = MAX_SENTENCE_LENGTH, trainable= True, mask_zero=True, name='word_embeddings',)(sentence_in)\n",
    "word_encoder = Bidirectional(GRU(100, return_sequences=True, kernel_regularizer=l2_reg))(embedded_word_seq)\n",
    "dense_transform_w = Dense(100,  name='dense_transform_w', kernel_regularizer=l2_reg)(word_encoder)\n",
    "attention_weighted_sentence = Model(sentence_in, Attention(name='word_attention', regularizer=l2_reg)(dense_transform_w))\n",
    "word_attention_model = attention_weighted_sentence\n",
    "\n",
    "\n",
    "texts_in = Input(shape=(MAX_SENTENCES, MAX_SENTENCE_LENGTH), dtype='int32')\n",
    "attention_weighted_sentences = TimeDistributed(attention_weighted_sentence)(texts_in)\n",
    "sentence_encoder = Bidirectional(GRU(100, return_sequences=True, kernel_regularizer=l2_reg))(attention_weighted_sentences)\n",
    "dense_transform_s = Dense(100,  name='dense_transform_s',kernel_regularizer=l2_reg)(sentence_encoder)\n",
    "attention_weighted_text = Attention(name='sentence_attention', regularizer=l2_reg)(dense_transform_s)\n",
    "prediction = Dense(y_train.shape[1], activation='softmax')(attention_weighted_text)\n",
    "model = Model(texts_in, prediction)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(momentum=0.9),loss='categorical_crossentropy',metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 10, 25)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 10, 100)           30140900  \n",
      "_________________________________________________________________\n",
      "bidirectional_16 (Bidirectio (None, 10, 200)           120600    \n",
      "_________________________________________________________________\n",
      "dense_transform_s (Dense)    (None, 10, 100)           20100     \n",
      "_________________________________________________________________\n",
      "sentence_attention (Attentio (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 17)                1717      \n",
      "=================================================================\n",
      "Total params: 30,283,417\n",
      "Trainable params: 30,283,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17730 samples, validate on 1970 samples\n",
      "Epoch 1/5\n",
      "17730/17730 [==============================] - 60s 3ms/step - loss: 2.7787 - acc: 0.0984 - val_loss: 2.7852 - val_acc: 0.0832\n",
      "Epoch 2/5\n",
      " 3744/17730 [=====>........................] - ETA: 45s - loss: 2.7832 - acc: 0.0978"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-7cfac934ea1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           validation_data=([x_val], \n\u001b[0;32m----> 8\u001b[0;31m                            [y_val]))\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(x=[x_train],\n",
    "          y=[y_train],\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          shuffle=True,\n",
    "          validation_data=([x_val], \n",
    "                           [y_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    reviews = []\n",
    "    texts = []\n",
    "    return kss.split_sentences(text) \n",
    "\n",
    "def encode_input(reviews):\n",
    "    \n",
    "    \n",
    "    \n",
    "    ret = np.zeros((MAX_SENTENCES, MAX_SENTENCE_LENGTH), dtype='int32')\n",
    "    for j, sent in enumerate(reviews):\n",
    "        if j < MAX_SENTENCES:\n",
    "            wordTokens = sp.EncodeAsIds(sent)\n",
    "            for k in range(len(wordTokens)):\n",
    "                if k< MAX_SENTENCE_LENGTH:\n",
    "                    ret[j][k] = wordTokens[k]\n",
    "            \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '공기업 비정규직의 정규화 그만해주세요. 만 하다가 처음으로 청원 글을 써봅니다. 공기업 비정규직의 정규직 전환 이제 그만해주십시오. 그간 한국도로공사 철도공사 서울교통공사 등등 많은 공기업들이 비정규직 정규화가 이루어졌습니다. 솔직히 비정규직 철폐라는 공약이 앞으로 비정규직 전형을 없애 채용하겠다던지, 해당 직렬의 자회사 정규직인 줄 알았습니다. 현실은 더 하더라구요.무분별한 비정규직의 정규화 당장 그만해야한다고 생각합니다.'\n",
    "text2 = '6월 12일 오후 1시 발생한 사건입니다. 시장 한복판에서 길고양이 학대사건이 발생했습니다. 임신한 고양이가 매장에 들어 이유만으로 줄에 묶여 던지고 목을 조르고 던졌습니다. 그 결과 그 임신한 고양이는 피를 토하며 괴로워했고 상인이 고양이를 박스에 담아 합니다. 시장 주변에서 그 고양이를 담아간 상자를 발견했다고 하는데 그 고양이를 어찌 했는지 모르는 상황입니다. 시장 **가게 근처 **가게 골목 입니다. 정말 임신한 고양이가 매장에 이유 만으로 던지고 줄로 묶어 목을 조르고 이런 행동에서 인간다운 인간성이 있다는 생각이 드는지 의문입니다. 이런 행동을 하는 사람은 사회에 위협이 될 수 있다고 생각합니다. 정확한 수사를 통해 엄벌에 하는 바램입니다. 최근에 동물 학대 사건과 관련하여 이례적인 판례도 나왔습니다. 이를 발판으로 동물보호법 강화를 통해 생명을 경시하는 일이 발생하지 않았으면 하는 바램입니다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_text = normalize(preprocess(text2))\n",
    "encoded_text = encode_input(normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  172,    74,   282, ...,     0,     0,     0],\n",
       "       [ 1364, 58802, 13982, ...,     0,     0,     0],\n",
       "       [28649, 52161, 39884, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,     0,     0,     0],\n",
       "       [    0,     0,     0, ...,     0,     0,     0],\n",
       "       [    0,     0,     0, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_text = normalize(preprocess(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6월 12일 오후 1시 발생한 사건입니다.',\n",
       " '시장 한복판에서 길고양이 학대사건이 발생했습니다.',\n",
       " '임신한 고양이가 매장에 들어 이유만으로 줄에 묶여 던지고 목을 조르고 던졌습니다.',\n",
       " '그 결과 그 임신한 고양이는 피를 토하며 괴로워했고 상인이 고양이를 박스에 담아 합니다.',\n",
       " '시장 주변에서 그 고양이를 담아간 상자를 발견했다고 하는데 그 고양이를 어찌 했는지 모르는 상황입니다.',\n",
       " '시장 가게 근처 가게 골목 입니다.',\n",
       " '정말 임신한 고양이가 매장에 이유 만으로 던지고 줄로 묶어 목을 조르고 이런 행동에서 인간다',\n",
       " '운 인간성이 있다는 생각이 드는지 의문입니다.',\n",
       " '이런 행동을 하는 사람은 사회에 위협이 될 수 있다고 생각합니다.',\n",
       " '정확한 수사를 통해 엄벌에 하는 바램입니다.',\n",
       " '최근에 동물 학대 사건과 관련하여 이례적인 판례도 나왔습니다.',\n",
       " '이를 발판으로 동물보호법 강화를 통해 생명을 경시하는 일이 발생하지 않았으면 하는 바램입니다.']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[172, 74, 282, 33, 2150, 34, 193, 1683, 5779, 3]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.EncodeAsIds(normalized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'월'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index[75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[184, 75, 240, 32, 2035, 33, 104, 1946, 6773, 3]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.EncodeAsIds('6월 12일 오후 1시 발생한 사건입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[172, 74, 282, 33, 2150, 34, 193, 1683, 5779, 3], [1364, 58802, 13982, 57630, 16, 11377, 3], [28649, 52161, 39884, 557, 2282, 447, 15, 7248, 28609, 10857, 66541, 30, 104006, 3], [25, 715, 25, 28649, 2823, 11, 2512, 123355, 34655, 2156, 26715, 16, 27728, 80032, 12430, 23, 3], [1364, 8348, 25, 27728, 12430, 1253, 8, 70201, 5535, 2586, 287, 25, 27728, 1614, 4822, 590, 897, 3], [1364, 2494, 7996, 2494, 14112, 86, 3], [88, 28649, 52161, 39884, 1078, 3823, 28609, 34231, 10227, 10857, 66541, 30, 66, 126472, 22986], [8533, 39283, 16, 280, 367, 22446, 3929, 3], [66, 1193, 38, 513, 1112, 8147, 137, 19, 171, 43, 3], [1462, 926, 155, 1503, 38, 6087, 3], [3629, 2509, 3580, 1950, 1273, 97967, 5394, 13, 3741, 3], [310, 74505, 12599, 6162, 155, 1135, 56517, 255, 3162, 4211, 38, 6087, 3], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "normalized_text = normalize(preprocess(text2))\n",
    "encoded_text = encode_input(normalized_text)\n",
    "\n",
    "hidden_word_encoding_out = Model(inputs=word_attention_model.input,outputs=word_attention_model.get_layer('dense_transform_w').output)\n",
    "hidden_word_encodings = hidden_word_encoding_out.predict(encoded_text)\n",
    "word_context = word_attention_model.get_layer('word_attention').get_weights()[0]\n",
    "u_wattention = encoded_text*np.exp(np.squeeze(np.dot(hidden_word_encodings, word_context)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nopad_encoded_text = encoded_text[-len(text):]\n",
    "nopad_encoded_text = [list(filter(lambda x: x > 0, sentence)) for sentence in nopad_encoded_text]\n",
    "print(nopad_encoded_text)\n",
    "\n",
    "\n",
    "\n",
    "reconstructed_texts = [[sp.DecodeIds([int(i)]) for i in sentence] for sentence in nopad_encoded_text]\n",
    "nopad_wattention = u_wattention[:len(normalized_text)]\n",
    "nopad_wattention = nopad_wattention/np.expand_dims(np.sum(nopad_wattention, -1), -1)\n",
    "nopad_wattention = np.array([attention_seq[:len(sentence)] for attention_seq, sentence in zip(nopad_wattention, nopad_encoded_text)])\n",
    "word_activation_maps = []\n",
    "\n",
    "\n",
    "for i, att in enumerate(nopad_wattention):\n",
    "    word_activation_maps.append(list(zip(reconstructed_texts[i], att)))\n",
    "\n",
    "hidden_sentence_encoding_out = Model(inputs=model.input, outputs=model.get_layer('dense_transform_s').output)\n",
    "hidden_sentence_encodings = np.squeeze(hidden_sentence_encoding_out.predict(np.expand_dims(encoded_text, 0)), 0)\n",
    "sentence_context = model.get_layer('sentence_attention').get_weights()[0]\n",
    "u_sattention = np.exp(np.squeeze(np.dot(hidden_sentence_encodings, sentence_context), -1))\n",
    "nopad_sattention = u_sattention[-len(normalized_text):]\n",
    "\n",
    "nopad_sattention = nopad_sattention/np.expand_dims(np.sum(nopad_sattention, -1), -1)\n",
    "\n",
    "activation_map = list(zip(word_activation_maps, nopad_sattention))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([('6', 0.016533692204171873),\n",
       "   ('월', 0.00711333269249255),\n",
       "   ('12', 0.027107565125444582),\n",
       "   ('일', 0.0031721618763818128),\n",
       "   ('오후', 0.2066711525521484),\n",
       "   ('1', 0.0032682879938479284),\n",
       "   ('시', 0.018552340670960298),\n",
       "   ('발생한', 0.16178025569547247),\n",
       "   ('사건입니다', 0.5555128328366817),\n",
       "   ('.', 0.0002883783523983466)],\n",
       "  0.083333336),\n",
       " ([('시장', 0.009526869403662677),\n",
       "   ('한복판에서', 0.41070306061156353),\n",
       "   ('길고양이', 0.09765739589590289),\n",
       "   ('학대사건', 0.40251721681310854),\n",
       "   ('이', 0.00011175213376730411),\n",
       "   ('발생했습니다', 0.07946275161691368),\n",
       "   ('.', 2.0953525081369522e-05)],\n",
       "  0.083333336),\n",
       " ([('임신한', 0.08394352000796979),\n",
       "   ('고양이가', 0.15283528036356284),\n",
       "   ('매장에', 0.11686283472365062),\n",
       "   ('들어', 0.0016320479124730068),\n",
       "   ('이유만으로', 0.006686415325428009),\n",
       "   ('줄', 0.0013097404252700791),\n",
       "   ('에', 4.3951020982217415e-05),\n",
       "   ('묶여', 0.021237133338607456),\n",
       "   ('던지고', 0.08382631728535053),\n",
       "   ('목을', 0.03181174898692896),\n",
       "   ('조르', 0.19496965914518194),\n",
       "   ('고', 8.790204196443483e-05),\n",
       "   ('던졌습니다', 0.30474465921843363),\n",
       "   ('.', 8.790204196443483e-06)],\n",
       "  0.083333336),\n",
       " ([('그', 7.312657039309919e-05),\n",
       "   ('결과', 0.002091419913242637),\n",
       "   ('그', 7.312657039309919e-05),\n",
       "   ('임신한', 0.08380012460767595),\n",
       "   ('고양이', 0.00825745232878876),\n",
       "   ('는', 3.217569097296364e-05),\n",
       "   ('피를', 0.007347757793098607),\n",
       "   ('토하며', 0.36082112363363006),\n",
       "   ('괴로워', 0.1013680518789141),\n",
       "   ('했고', 0.006306435430700874),\n",
       "   ('상인', 0.07814305312206579),\n",
       "   ('이', 4.680100505158348e-05),\n",
       "   ('고양이를', 0.08110614175439418),\n",
       "   ('박스에', 0.23409862726802058),\n",
       "   ('담아', 0.03635853079944892),\n",
       "   ('합니다', 6.727644476165125e-05),\n",
       "   ('.', 8.775188447171902e-06)],\n",
       "  0.083333336),\n",
       " ([('시장', 0.008244481516404343),\n",
       "   ('주변에서', 0.050458161069606634),\n",
       "   ('그', 0.0001511085321921617),\n",
       "   ('고양이를', 0.1675974952249704),\n",
       "   ('담아', 0.07513116220594279),\n",
       "   ('간', 0.007573559633471144),\n",
       "   ('', 4.8354730301491744e-05),\n",
       "   ('상자를', 0.4243188027368777),\n",
       "   ('발견', 0.0334554290273446),\n",
       "   ('했다고', 0.015630666569957204),\n",
       "   ('하는데', 0.0017347259495660162),\n",
       "   ('그', 0.0001511085321921617),\n",
       "   ('고양이를', 0.1675974952249704),\n",
       "   ('어찌', 0.00975556683832596),\n",
       "   ('했는지', 0.02914581368922415),\n",
       "   ('모르는', 0.003566161359735016),\n",
       "   ('상황입니다', 0.0054217741350547615),\n",
       "   ('.', 1.8133023863059404e-05)],\n",
       "  0.083333336),\n",
       " ([('시장', 0.04777750534169323),\n",
       "   ('가게', 0.0873585764825388),\n",
       "   ('근처', 0.2800798626922134),\n",
       "   ('가게', 0.0873585764825388),\n",
       "   ('골목', 0.4943080318049669),\n",
       "   ('입니다', 0.0030123647062944413),\n",
       "   ('.', 0.00010508248975445725)],\n",
       "  0.083333336),\n",
       " ([('정말', 0.00020671737506518643),\n",
       "   ('임신한', 0.06729825088911962),\n",
       "   ('고양이가', 0.12252937500880898),\n",
       "   ('매장에', 0.09368995212613518),\n",
       "   ('이유', 0.002532287844548534),\n",
       "   ('만으로', 0.008980460509934179),\n",
       "   ('던지고', 0.06720428844590817),\n",
       "   ('줄로', 0.08041070983927724),\n",
       "   ('묶어', 0.024023847668087066),\n",
       "   ('목을', 0.025503756148667376),\n",
       "   ('조르', 0.15630887334332466),\n",
       "   ('고', 7.047183240858628e-05),\n",
       "   ('이런', 0.00015503803129888983),\n",
       "   ('행동에서', 0.2970904529459575),\n",
       "   ('인간다', 0.05399551799145881)],\n",
       "  0.083333336),\n",
       " ([('운', 0.11399067555472434),\n",
       "   ('인간성', 0.5247739022402714),\n",
       "   ('이', 0.00021374086591768304),\n",
       "   ('있다는', 0.0037404651535594535),\n",
       "   ('생각이', 0.0049026811119868546),\n",
       "   ('드는지', 0.2998517172742696),\n",
       "   ('의문입니다', 0.05248674138691104),\n",
       "   ('.', 4.0076412359565575e-05)],\n",
       "  0.083333336),\n",
       " ([('이런', 0.0057682223387519665),\n",
       "   ('행동을', 0.10426498863834995),\n",
       "   ('하는', 0.003321097710190526),\n",
       "   ('사람은', 0.0448348190875721),\n",
       "   ('사회에', 0.09718580667715435),\n",
       "   ('위협이', 0.7120258696032162),\n",
       "   ('될', 0.011973431218318476),\n",
       "   ('수', 0.001660548855095263),\n",
       "   ('있다고', 0.014944939695857367),\n",
       "   ('생각합니다', 0.0037580842510050692),\n",
       "   ('.', 0.00026219192448872575)],\n",
       "  0.083333336),\n",
       " ([('정확한', 0.14369962649891882),\n",
       "   ('수사를', 0.0910163160998624),\n",
       "   ('통해', 0.015234912522115196),\n",
       "   ('엄벌에', 0.1477295065854138),\n",
       "   ('하는', 0.0037350108118734027),\n",
       "   ('바램입니다', 0.5982897582071948),\n",
       "   ('.', 0.00029486927462158444)],\n",
       "  0.083333336),\n",
       " ([('최근에', 0.030226805154132553),\n",
       "   ('동물', 0.02089805845459316),\n",
       "   ('학대', 0.0298186724860277),\n",
       "   ('사건과', 0.01624201434294805),\n",
       "   ('관련하여', 0.01060312013260147),\n",
       "   ('이례적인', 0.8159904713515854),\n",
       "   ('판례', 0.04492791044403169),\n",
       "   ('도', 0.00010828009561965367),\n",
       "   ('나왔습니다', 0.03115967982408649),\n",
       "   ('.', 2.498771437376623e-05)],\n",
       "  0.083333336),\n",
       " ([('이를', 0.0018772064745456857),\n",
       "   ('발판으로', 0.4511653818904075),\n",
       "   ('동물보호법', 0.07629330442839063),\n",
       "   ('강화를', 0.037314020310162954),\n",
       "   ('통해', 0.0009386032372728429),\n",
       "   ('생명을', 0.0068729978987398495),\n",
       "   ('경시하는', 0.34223896232870493),\n",
       "   ('일이', 0.0015441537129327414),\n",
       "   ('발생하지', 0.019147506040365993),\n",
       "   ('않았으면', 0.02549973053003833),\n",
       "   ('하는', 0.0002301091807507615),\n",
       "   ('바램입니다', 0.03685985745341803),\n",
       "   ('.', 1.816651426979696e-05)],\n",
       "  0.083333336)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = model.predict(np.reshape(encoded_text, (1,)+encoded_text.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05934172, 0.05819092, 0.05916221, 0.0580809 , 0.05894235,\n",
       "        0.05866391, 0.05869587, 0.05889405, 0.05854826, 0.05894068,\n",
       "        0.05893498, 0.05891773, 0.05885287, 0.05884808, 0.05846177,\n",
       "        0.05920391, 0.05931978]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  1, 14,  8,  5,  6, 13, 12,  7, 11, 10,  9,  4,  2, 15, 16,  0])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.argsort()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['미래',\n",
       " '육아/교육',\n",
       " '반려동물',\n",
       " '경제민주화',\n",
       " '농산어촌',\n",
       " '행정',\n",
       " '보건복지',\n",
       " '기타',\n",
       " '교통/건축/국토',\n",
       " '성장동력',\n",
       " '정치개혁',\n",
       " '일자리',\n",
       " '저출산/고령화대책',\n",
       " '외교/통일/국방',\n",
       " '인권/성평등',\n",
       " '문화/예술/체육/언론',\n",
       " '안전/환경']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo = pd.read_csv('yahoo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_x= pd.DataFrame(yahoo.ko)\n",
    "yahoo_y = pd.DataFrame(yahoo.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_reviews = []\n",
    "yahoo_texts = []\n",
    "for text in yahoo_x.ko:\n",
    "    sentences =kss.split_sentences(text)\n",
    "    for sen in sentences:\n",
    "        yahoo_texts.append(sen)\n",
    "    yahoo_reviews.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(yahoo_texts)\n",
    "MAX_SENTENCES = 10\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "yahoo_X_train = np.zeros((max_nb_instances, MAX_SENTENCES, MAX_SENTENCE_LENGTH), dtype='int32')\n",
    "for i, sentences in enumerate(yahoo_reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTENCES:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENTENCE_LENGTH:\n",
    "                    X_train[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_Y_train = to_categorical(np.asarray(yahoo_y))\n",
    "\n",
    "validation_split = 0.1\n",
    "nb_validation_samples = int(validation_split * yahoo_X_train.shape[0])\n",
    "\n",
    "yahoo_x_train = yahoo_X_train[:-nb_validation_samples]\n",
    "yahoo_y_train = yahoo_Y_train[:-nb_validation_samples]\n",
    "yahoo_x_val = yahoo_X_train[-nb_validation_samples:]\n",
    "yahoo_y_val = yahoo_Y_train[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = regularizers.l2(1e-8)\n",
    "\n",
    "sentence_in = Input(shape=(MAX_SENTENCE_LENGTH,), dtype='int32')\n",
    "embedded_word_seq = Embedding(len(word_index) +1,embedding_dim, weights=[embedding_matrix], input_length = MAX_SENTENCE_LENGTH, trainable= True, mask_zero=True, name='word_embeddings',)(sentence_in)\n",
    "word_encoder = Bidirectional(GRU(50, return_sequences=True, kernel_regularizer=l2_reg))(embedded_word_seq)\n",
    "dense_transform_w = Dense(100, activation='relu', name='dense_transform_w', kernel_regularizer=l2_reg)(word_encoder)\n",
    "attention_weighted_sentence = Model(sentence_in, Attention(name='word_attention', regularizer=l2_reg)(dense_transform_w))\n",
    "word_attention_model = attention_weighted_sentence\n",
    "\n",
    "\n",
    "texts_in = Input(shape=(MAX_SENTENCES, MAX_SENTENCE_LENGTH), dtype='int32')\n",
    "attention_weighted_sentences = TimeDistributed(attention_weighted_sentence)(texts_in)\n",
    "sentence_encoder = Bidirectional(GRU(50, return_sequences=True, kernel_regularizer=l2_reg))(attention_weighted_sentences)\n",
    "dense_transform_s = Dense(100, activation='relu', name='dense_transform_s',kernel_regularizer=l2_reg)(sentence_encoder)\n",
    "attention_weighted_text = Attention(name='sentence_attention', regularizer=l2_reg)(dense_transform_s)\n",
    "yahoo_prediction = Dense(yahoo_y_train.shape[1], activation='softmax')(attention_weighted_text)\n",
    "yahoo_model = Model(texts_in, yahoo_prediction)\n",
    "\n",
    "yahoo_model.compile(optimizer=keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yahoo_model.fit(x=[yahoo_x_train],\n",
    "          y=[yahoo_y_train],\n",
    "          batch_size=512,\n",
    "          epochs=10,\n",
    "          shuffle=True,\n",
    "          validation_data=([yahoo_x_val], \n",
    "                           [yahoo_y_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model2 = Word2Vec.load('ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    temp = get_vector(word, w2v_model2)\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
