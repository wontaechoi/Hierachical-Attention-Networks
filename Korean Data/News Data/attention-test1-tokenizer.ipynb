{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from konlpy.tag import Kkma, Okt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import kss\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sn\n",
    "import matplotlib.font_manager as fm\n",
    "fm.get_fontconfig_fonts()\n",
    "font_location = '/home/bigdata/jupyter_notebook/wontae/NanumGothic.ttf' # For Windows\n",
    "font_name = fm.FontProperties(fname=font_location)\n",
    "plt.rc('font', family = font_name.get_name())\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from keras import regularizers\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input, RepeatVector, Permute, Multiply, Lambda,BatchNormalization\n",
    "from keras.layers import Embedding, GRU, Bidirectional, TimeDistributed, LSTM, Dropout\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from nltk import tokenize\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, regularizer = None, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.regularizer = regularizer\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.context = self.add_weight(name='context', shape=(input_shape[-1], 1),initializer=initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None),regularizer=self.regularizer,trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        attention_in = K.exp(K.squeeze(K.dot(x, self.context), axis=-1))\n",
    "        attention = attention_in/K.expand_dims(K.sum(attention_in, axis=-1), -1)\n",
    "\n",
    "        if mask is not None:\n",
    "        # use only the inputs specified by the mask\n",
    "        # import pdb; pdb.set_trace()\n",
    "            attention = attention*K.cast(mask, 'float32')\n",
    "\n",
    "        weighted_sum = K.batch_dot(K.permute_dimensions(x, [0, 2, 1]), attention)\n",
    "        return weighted_sum\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul  1 10:06:55 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 396.37                 Driver Version: 396.37                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |\r\n",
      "| N/A   29C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:D8:00.0 Off |                    0 |\r\n",
      "| N/A   27C    P0    32W / 250W |    323MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    1    332799      C   /home/bigdata/.conda/envs/py36/bin/python    313MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "petition_data = pd.read_csv('petition.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_content_combine(title,content):\n",
    "    return title + ' ' + str(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "petition_data['petition'] = petition_data.apply(lambda x : title_content_combine(x.title, x.content), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = petition_data[['petition', 'category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nb_instances = 100000\n",
    "x  = pd.DataFrame(data.petition)\n",
    "y = pd.DataFrame(data.category)\n",
    "y.columns = ['category']\n",
    "x_train = x[:max_nb_instances]\n",
    "y_train = y[:max_nb_instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace('\\n', '')\n",
    "    t = re.sub('[^0-9a-zA-Zㄱ-힗,.!?\"\"''\\s]', '', text)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['mod_petition'] = x.apply(lambda j : preprocess(j.petition), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x.petition[:max_nb_instances]\n",
    "y_train = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "MAX_NB_WORDS = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "texts = []\n",
    "for text in x.petition[:max_nb_instances]:\n",
    "    sentences =kss.split_sentences(text)\n",
    "    for sen in sentences:\n",
    "        texts.append(sen)\n",
    "    reviews.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCES = 10\n",
    "MAX_SENTENCE_LENGTH = 20\n",
    "X_train = np.zeros((max_nb_instances, MAX_SENTENCES, MAX_SENTENCE_LENGTH), dtype='int32')\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTENCES:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENTENCE_LENGTH:\n",
    "                    X_train[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1241725 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_to_number(y_data):\n",
    "    category_list = list(set(y_data.category))\n",
    "    new_y = []\n",
    "    for i, category in enumerate(y_data.category):\n",
    "        new_y.append(category_list.index(category))\n",
    "    ret = pd.DataFrame(new_y)\n",
    "    ret.columns = ['category']\n",
    "    return ret,category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.columns = ['category']\n",
    "Y_train,category_list = category_to_number(y_train)\n",
    "Y_train = to_categorical(np.asarray(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validation_split = 0.1\n",
    "nb_validation_samples = int(validation_split * X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train[:-nb_validation_samples]\n",
    "y_train = Y_train[:-nb_validation_samples]\n",
    "x_val = X_train[-nb_validation_samples:]\n",
    "y_val = Y_train[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i, pet in enumerate(x.mod_petition[:max_nb_instances]):\n",
    "    if i%5000==0: # 5,000의 배수로 While문이 실행될 때마다 몇 번째 While문 실행인지 출력.\n",
    "        print(\"%d번째 for문.\"%i)\n",
    "    tokenlist = okt.pos(pet, stem=True, norm=True) # 단어 토큰화\n",
    "    temp=[]\n",
    "    for word in tokenlist:\n",
    "        if word[1] in [\"Noun\"]: # 명사일 때만\n",
    "            temp.append((word[0])) # 해당 단어를 저장함\n",
    "\n",
    "    if temp: # 만약 이번에 읽은 데이터에 명사가 존재할 경우에만\n",
    "      result.append(temp) # 결과에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(result, size=embedding_dim, window=10, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('han_test_w2v-pre.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load('han_test_w2v.model')\n",
    "w2v_model = Word2Vec.load('han_test_w2v-pre.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word, word2vec_model):\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    temp = get_vector(word, w2v_model)\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = regularizers.l2(1e-8)\n",
    "\n",
    "sentence_in = Input(shape=(MAX_SENTENCE_LENGTH,), dtype='int32')\n",
    "embedded_word_seq = Embedding(len(word_index) +1,embedding_dim, weights=[embedding_matrix], input_length = MAX_SENTENCE_LENGTH, trainable= True, mask_zero=True, name='word_embeddings',)(sentence_in)\n",
    "word_encoder = Bidirectional(GRU(50, return_sequences=True, kernel_regularizer=l2_reg))(embedded_word_seq)\n",
    "dense_transform_w = Dense(100, activation='relu', name='dense_transform_w', kernel_regularizer=l2_reg)(word_encoder)\n",
    "attention_weighted_sentence = Model(sentence_in, Attention(name='word_attention', regularizer=l2_reg)(dense_transform_w))\n",
    "word_attention_model = attention_weighted_sentence\n",
    "\n",
    "\n",
    "texts_in = Input(shape=(MAX_SENTENCES, MAX_SENTENCE_LENGTH), dtype='int32')\n",
    "attention_weighted_sentences = TimeDistributed(attention_weighted_sentence)(texts_in)\n",
    "sentence_encoder = Bidirectional(GRU(50, return_sequences=True, kernel_regularizer=l2_reg))(attention_weighted_sentences)\n",
    "dense_transform_s = Dense(100, activation='relu', name='dense_transform_s',kernel_regularizer=l2_reg)(sentence_encoder)\n",
    "attention_weighted_text = Attention(name='sentence_attention', regularizer=l2_reg)(dense_transform_s)\n",
    "prediction = Dense(y_train.shape[1], activation='softmax')(attention_weighted_text)\n",
    "model = Model(texts_in, prediction)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 10, 20)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 10, 100)           124228100 \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 10, 100)           45300     \n",
      "_________________________________________________________________\n",
      "dense_transform_s (Dense)    (None, 10, 100)           10100     \n",
      "_________________________________________________________________\n",
      "sentence_attention (Attentio (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 17)                1717      \n",
      "=================================================================\n",
      "Total params: 124,285,317\n",
      "Trainable params: 124,285,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:109: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 124172600 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/bigdata/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:109: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 124172600 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "90000/90000 [==============================] - 34s 375us/step - loss: 2.0448 - acc: 0.3572 - val_loss: 1.9656 - val_acc: 0.3891\n",
      "Epoch 2/10\n",
      "90000/90000 [==============================] - 25s 278us/step - loss: 1.3145 - acc: 0.5824 - val_loss: 1.8715 - val_acc: 0.4239\n",
      "Epoch 3/10\n",
      "90000/90000 [==============================] - 25s 279us/step - loss: 0.6337 - acc: 0.8090 - val_loss: 2.3008 - val_acc: 0.3893\n",
      "Epoch 4/10\n",
      "90000/90000 [==============================] - 25s 279us/step - loss: 0.3110 - acc: 0.9080 - val_loss: 2.7088 - val_acc: 0.3686\n",
      "Epoch 5/10\n",
      "90000/90000 [==============================] - 25s 276us/step - loss: 0.2066 - acc: 0.9364 - val_loss: 2.9960 - val_acc: 0.3622\n",
      "Epoch 6/10\n",
      "90000/90000 [==============================] - 25s 274us/step - loss: 0.1649 - acc: 0.9479 - val_loss: 3.1319 - val_acc: 0.3565\n",
      "Epoch 7/10\n",
      "90000/90000 [==============================] - 25s 274us/step - loss: 0.1429 - acc: 0.9541 - val_loss: 3.3322 - val_acc: 0.3598\n",
      "Epoch 8/10\n",
      "90000/90000 [==============================] - 25s 275us/step - loss: 0.1297 - acc: 0.9577 - val_loss: 3.3719 - val_acc: 0.3525\n",
      "Epoch 9/10\n",
      "90000/90000 [==============================] - 25s 273us/step - loss: 0.1204 - acc: 0.9604 - val_loss: 3.4721 - val_acc: 0.3589\n",
      "Epoch 10/10\n",
      "90000/90000 [==============================] - 25s 276us/step - loss: 0.1135 - acc: 0.9619 - val_loss: 3.5643 - val_acc: 0.3524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f766df0e390>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(x=[x_train],\n",
    "          y=[y_train],\n",
    "          batch_size=512,\n",
    "          epochs=10,\n",
    "          shuffle=True,\n",
    "          validation_data=([x_val], \n",
    "                           [y_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    reviews = []\n",
    "    texts = []\n",
    "    return kss.split_sentences(text) \n",
    "\n",
    "def encode_input(reviews):\n",
    "    \n",
    "    \n",
    "    \n",
    "    ret = np.zeros((MAX_SENTENCES, MAX_SENTENCE_LENGTH), dtype='int32')\n",
    "    for j, sent in enumerate(reviews):\n",
    "        if j < MAX_SENTENCES:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENTENCE_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    ret[j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '공기업 비정규직의 정규화 그만해주세요. 만 하다가 처음으로 청원 글을 써봅니다. 공기업 비정규직의 정규직 전환 이제 그만해주십시오. 그간 한국도로공사 철도공사 서울교통공사 등등 많은 공기업들이 비정규직 정규화가 이루어졌습니다. 솔직히 비정규직 철폐라는 공약이 앞으로 비정규직 전형을 없애 채용하겠다던지, 해당 직렬의 자회사 정규직인 줄 알았습니다. 현실은 더 하더라구요.무분별한 비정규직의 정규화 당장 그만해야한다고 생각합니다.'\n",
    "text2 = '6월 12일 오후 1시 발생한 사건입니다. 시장 한복판에서 길고양이 학대사건이 발생했습니다. 임신한 고양이가 매장에 들어 이유만으로 줄에 묶여 던지고 목을 조르고 던졌습니다. 그 결과 그 임신한 고양이는 피를 토하며 괴로워했고 상인이 고양이를 박스에 담아 합니다. 시장 주변에서 그 고양이를 담아간 상자를 발견했다고 하는데 그 고양이를 어찌 했는지 모르는 상황입니다. 시장 **가게 근처 **가게 골목 입니다. 정말 임신한 고양이가 매장에 이유 만으로 던지고 줄로 묶어 목을 조르고 이런 행동에서 인간다운 인간성이 있다는 생각이 드는지 의문입니다. 이런 행동을 하는 사람은 사회에 위협이 될 수 있다고 생각합니다. 정확한 수사를 통해 엄벌에 하는 바램입니다. 최근에 동물 학대 사건과 관련하여 이례적인 판례도 나왔습니다. 이를 발판으로 동물보호법 강화를 통해 생명을 경시하는 일이 발생하지 않았으면 하는 바램입니다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_text = normalize(preprocess(text2))\n",
    "encoded_text = encode_input(normalized_text)\n",
    "\n",
    "hidden_word_encoding_out = Model(inputs=word_attention_model.input,outputs=word_attention_model.get_layer('dense_transform_w').output)\n",
    "hidden_word_encodings = hidden_word_encoding_out.predict(encoded_text)\n",
    "word_context = word_attention_model.get_layer('word_attention').get_weights()[0]\n",
    "u_wattention = encoded_text*np.exp(np.squeeze(np.dot(hidden_word_encodings, word_context)))\n",
    "\n",
    "\n",
    "reverse_word_index = {value:key for key,value in tokenizer.word_index.items()}\n",
    "\n",
    "\n",
    "\n",
    "nopad_encoded_text = encoded_text[-len(text):]\n",
    "nopad_encoded_text = [list(filter(lambda x: x > 0, sentence)) for sentence in nopad_encoded_text]\n",
    "reconstructed_texts = [[reverse_word_index[int(i)] for i in sentence] for sentence in nopad_encoded_text]\n",
    "nopad_wattention = u_wattention[:len(normalized_text)]\n",
    "nopad_wattention = nopad_wattention/np.expand_dims(np.sum(nopad_wattention, -1), -1)\n",
    "nopad_wattention = np.array([attention_seq[:len(sentence)] for attention_seq, sentence in zip(nopad_wattention, nopad_encoded_text)])\n",
    "word_activation_maps = []\n",
    "\n",
    "\n",
    "for i, att in enumerate(nopad_wattention):\n",
    "    word_activation_maps.append(list(zip(reconstructed_texts[i], att)))\n",
    "\n",
    "hidden_sentence_encoding_out = Model(inputs=model.input, outputs=model.get_layer('dense_transform_s').output)\n",
    "hidden_sentence_encodings = np.squeeze(hidden_sentence_encoding_out.predict(np.expand_dims(encoded_text, 0)), 0)\n",
    "sentence_context = model.get_layer('sentence_attention').get_weights()[0]\n",
    "u_sattention = np.exp(np.squeeze(np.dot(hidden_sentence_encodings, sentence_context), -1))\n",
    "nopad_sattention = u_sattention[-len(normalized_text):]\n",
    "\n",
    "nopad_sattention = nopad_sattention/np.expand_dims(np.sum(nopad_sattention, -1), -1)\n",
    "\n",
    "activation_map = list(zip(word_activation_maps, nopad_sattention))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([('6월', 0.04168116390776455),\n",
       "   ('12일', 0.01932456377925637),\n",
       "   ('오후', 0.0008100221776735624),\n",
       "   ('1시', 0.6243790887451769),\n",
       "   ('발생한', 0.21404099341761987),\n",
       "   ('사건입니다', 0.0997641679725087)],\n",
       "  0.12730905),\n",
       " ([('시장', 0.0019156067798971577),\n",
       "   ('한복판에서', 0.5034924683751569),\n",
       "   ('길고양이', 0.49417098075441435),\n",
       "   ('발생했습니다', 0.0004209440905315677)],\n",
       "  0.14146172),\n",
       " ([('임신한', 0.3506479820393029),\n",
       "   ('고양이가', 0.45706276905742343),\n",
       "   ('매장에', 0.12958215881449828),\n",
       "   ('들어', 0.0002876960370728489),\n",
       "   ('이유만으로', 0.0007544380184230185),\n",
       "   ('묶여', 0.002569812492789212),\n",
       "   ('던지고', 0.00965307341876743),\n",
       "   ('목을', 0.012568667527527184),\n",
       "   ('던졌습니다', 0.036873402594195664)],\n",
       "  0.11549062),\n",
       " ([('그', 3.0514275381663843e-10),\n",
       "   ('결과', 2.208520112826515e-07),\n",
       "   ('그', 8.96776096155476e-09),\n",
       "   ('임신한', 0.048438409394575194),\n",
       "   ('고양이는', 0.45110201847139947),\n",
       "   ('피를', 0.007482566760458755),\n",
       "   ('고양이를', 0.08208902734610037),\n",
       "   ('박스에', 0.3919827582617525),\n",
       "   ('담아', 0.0189049764165638),\n",
       "   ('합니다', 1.3224235005076436e-08)],\n",
       "  0.13153116),\n",
       " ([('시장', 0.02523296480452186),\n",
       "   ('주변에서', 0.0432237392387526),\n",
       "   ('그', 2.2383916345350788e-07),\n",
       "   ('고양이를', 0.150589998550749),\n",
       "   ('하는데', 0.0010835802154289388),\n",
       "   ('그', 2.589684412236862e-07),\n",
       "   ('고양이를', 0.6339440985057164),\n",
       "   ('어찌', 0.00011469098548036299),\n",
       "   ('했는지', 0.14368407307960931),\n",
       "   ('모르는', 0.0018646991867071166),\n",
       "   ('상황입니다', 0.0002616726254297041)],\n",
       "  0.023608824),\n",
       " ([('시장', 0.10901766222512978),\n",
       "   ('가게', 0.0024805843065791497),\n",
       "   ('근처', 0.1498071771772423),\n",
       "   ('가게', 0.004283750358617662),\n",
       "   ('골목', 0.6933504133588958),\n",
       "   ('입니다', 0.04106041257353533)],\n",
       "  0.011190527),\n",
       " ([('정말', 3.881949542803708e-08),\n",
       "   ('임신한', 0.09538387880135143),\n",
       "   ('고양이가', 0.4984049623178323),\n",
       "   ('매장에', 0.3751663734143352),\n",
       "   ('이유', 3.057882165039407e-05),\n",
       "   ('만으로', 0.00034422936494744405),\n",
       "   ('던지고', 0.0037722225086337428),\n",
       "   ('줄로', 0.014213422027661241),\n",
       "   ('묶어', 0.001645987097195283),\n",
       "   ('목을', 0.008229352725787186),\n",
       "   ('이런', 9.216284305463613e-06),\n",
       "   ('인간다', 0.0027997378168048486)],\n",
       "  0.22181803),\n",
       " ([('운', 0.20933763204579958),\n",
       "   ('있다는', 0.010808481867504265),\n",
       "   ('생각이', 0.002888129618477022),\n",
       "   ('드는지', 0.6677260031291865),\n",
       "   ('의문입니다', 0.10923975333903262)],\n",
       "  0.017915031),\n",
       " ([('이런', 0.005159375354257311),\n",
       "   ('행동을', 0.6227936121409039),\n",
       "   ('하는', 0.005984189718245222),\n",
       "   ('사람은', 0.06992377995805919),\n",
       "   ('사회에', 0.031476110336612934),\n",
       "   ('위협이', 0.2643525194974017),\n",
       "   ('될', 0.00019541660586240244),\n",
       "   ('수', 2.763677285148784e-08),\n",
       "   ('있다고', 0.00010934473799357264),\n",
       "   ('생각합니다', 5.624013890974717e-06)],\n",
       "  0.19271341),\n",
       " ([('정확한', 0.04462225905202534),\n",
       "   ('수사를', 0.002026610005234486),\n",
       "   ('통해', 1.723972273855521e-05),\n",
       "   ('엄벌에', 0.7444938745522203),\n",
       "   ('하는', 0.0006844662626991948),\n",
       "   ('바램입니다', 0.20815555040508218)],\n",
       "  0.01696162)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = model.predict(np.reshape(encoded_text, (1,)+encoded_text.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 10, 11,  1, 14,  3,  6, 15,  5,  4, 12, 16,  7,  2,  8,  0,  9])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.argsort()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['미래',\n",
       " '육아/교육',\n",
       " '반려동물',\n",
       " '경제민주화',\n",
       " '농산어촌',\n",
       " '행정',\n",
       " '보건복지',\n",
       " '기타',\n",
       " '교통/건축/국토',\n",
       " '성장동력',\n",
       " '정치개혁',\n",
       " '일자리',\n",
       " '저출산/고령화대책',\n",
       " '외교/통일/국방',\n",
       " '인권/성평등',\n",
       " '문화/예술/체육/언론',\n",
       " '안전/환경']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo = pd.read_csv('yahoo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_x= pd.DataFrame(yahoo.ko)\n",
    "yahoo_y = pd.DataFrame(yahoo.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_reviews = []\n",
    "yahoo_texts = []\n",
    "for text in yahoo_x.ko:\n",
    "    sentences =kss.split_sentences(text)\n",
    "    for sen in sentences:\n",
    "        yahoo_texts.append(sen)\n",
    "    yahoo_reviews.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(yahoo_texts)\n",
    "MAX_SENTENCES = 10\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "yahoo_X_train = np.zeros((max_nb_instances, MAX_SENTENCES, MAX_SENTENCE_LENGTH), dtype='int32')\n",
    "for i, sentences in enumerate(yahoo_reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTENCES:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENTENCE_LENGTH:\n",
    "                    X_train[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_Y_train = to_categorical(np.asarray(yahoo_y))\n",
    "\n",
    "validation_split = 0.1\n",
    "nb_validation_samples = int(validation_split * yahoo_X_train.shape[0])\n",
    "\n",
    "yahoo_x_train = yahoo_X_train[:-nb_validation_samples]\n",
    "yahoo_y_train = yahoo_Y_train[:-nb_validation_samples]\n",
    "yahoo_x_val = yahoo_X_train[-nb_validation_samples:]\n",
    "yahoo_y_val = yahoo_Y_train[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = regularizers.l2(1e-8)\n",
    "\n",
    "sentence_in = Input(shape=(MAX_SENTENCE_LENGTH,), dtype='int32')\n",
    "embedded_word_seq = Embedding(len(word_index) +1,embedding_dim, weights=[embedding_matrix], input_length = MAX_SENTENCE_LENGTH, trainable= True, mask_zero=True, name='word_embeddings',)(sentence_in)\n",
    "word_encoder = Bidirectional(GRU(50, return_sequences=True, kernel_regularizer=l2_reg))(embedded_word_seq)\n",
    "dense_transform_w = Dense(100, activation='relu', name='dense_transform_w', kernel_regularizer=l2_reg)(word_encoder)\n",
    "attention_weighted_sentence = Model(sentence_in, Attention(name='word_attention', regularizer=l2_reg)(dense_transform_w))\n",
    "word_attention_model = attention_weighted_sentence\n",
    "\n",
    "\n",
    "texts_in = Input(shape=(MAX_SENTENCES, MAX_SENTENCE_LENGTH), dtype='int32')\n",
    "attention_weighted_sentences = TimeDistributed(attention_weighted_sentence)(texts_in)\n",
    "sentence_encoder = Bidirectional(GRU(50, return_sequences=True, kernel_regularizer=l2_reg))(attention_weighted_sentences)\n",
    "dense_transform_s = Dense(100, activation='relu', name='dense_transform_s',kernel_regularizer=l2_reg)(sentence_encoder)\n",
    "attention_weighted_text = Attention(name='sentence_attention', regularizer=l2_reg)(dense_transform_s)\n",
    "yahoo_prediction = Dense(yahoo_y_train.shape[1], activation='softmax')(attention_weighted_text)\n",
    "yahoo_model = Model(texts_in, yahoo_prediction)\n",
    "\n",
    "yahoo_model.compile(optimizer=keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yahoo_model.fit(x=[yahoo_x_train],\n",
    "          y=[yahoo_y_train],\n",
    "          batch_size=512,\n",
    "          epochs=10,\n",
    "          shuffle=True,\n",
    "          validation_data=([yahoo_x_val], \n",
    "                           [yahoo_y_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model2 = Word2Vec.load('ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    temp = get_vector(word, w2v_model2)\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
